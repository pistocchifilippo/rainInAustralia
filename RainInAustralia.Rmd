---
title: "RainInAustralia"
author: "Filippo, Antoni, Cristina, Mengxue"
date: "6/11/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path='Figs/', echo=FALSE, warning=FALSE, message=FALSE)
```


```{r}
library(kableExtra)
library(stats)
```

# Problem description

The problem is to predict whether it will rain the next day or not based on measurements taken by the australian's government meteorological department. The measurements have been taken daily in various regions of Australia, with over 10 years of measurements. Due to the nature of the instruments used, some measurements are missing heavily in some areas, or are mostly incomplete. 

The purpose of our work is to find insights into the weather data, to see if we can ascend some patterns that might help better understand rain prediction.

# Dataset description

The dataset is compromised of 23 variables, and is a timeseries of australian weather, which the purpose of predicting whether it would rain tomorrow.

* **Date** - Categorical variable, when the measurements were taken
* **Location** - Categorical variable, where the measurements were taken
* **MinTemp** - Numerical variable, minimal temperature observed that day
* **MaxTemp** - Numerical variable, maximal temperature observed that day
* **Rainfall** - Numerical variable, precipitation in the 24hours to 9am
* **Evaporation** - Numerical variable, "Class A" pan evaporation in the 24 hours to 9am
* **Sunshine** - Numerical variablem, bright sunshine hours in the 24 hours to midnight
* **WindGustDir** - Categorical variable, direction of strongest gust in the 24 hours to midnight, 16 compass points
* **WindGustSpeed** - Numerical variable, speed of strongest wind gust in the 24 hours to midnight
* **WindDir9am** - Categorical variable, wind direction averaged over 10 minutes prior to 9 am
* **WindDir3pm** - Categorical variable, wind direction averaged over 10 minutes prior to 3 pm
* **WindSpeed9am** - Numerical variable, wind speed averaged over 10 minutes prior to 9 am
* **Windspeed3pm** - Numerical variable, wind speed averaged over 10 minutes prior to 9 am
* **Humidity9am** - Numerical variable, relative humidity at 9 am
* **Humidity3pm** - Numerical variable, relative humidity at 3 pm
* **Pressure9am** - Numerical variable, atmospheric pressure reduced to mean sea level at 9 am
* **Pressure3pm** - Numerical variable, atmospheric pressure reduced to mean sea level at 3 pm
* **Cloud9am** - Numerical variable, fraction of sky obscured by cloud at 9 am
* **Cloud3pm** - Numerical variable, fraction of sky obscured by cloud at 3 pm
* **Temp9am** - Numerical variable, temperature in Celsius at 9am
* **Temp3pm** - Numerical variable, temperature in Celsius at 3pm
* **RainToday** - Categorical variable, whether it rained today or not
* **RainTomorrow** - Categorical variable, whether tomorrow will rain or not

```{r}

set.seed(1)

australianWeather <- read.csv(file = 'weatherAUS.csv')
```

# Preprocessing

We perform a basic visualization, first of the correlation between variables, which isn't significant with the exception of variables recorded in the same day, that is, those measurements taken at 9am and 3pm, this helps us see that there's an important temporal component in the same day.
```{r}
library(naniar)
library(ggplot2)
library(reshape2)
library(dplyr)
library(timelineR)
library(tseries)

nums <- unlist(lapply(australianWeather, is.numeric))  

australianWeather$Date=as.POSIXct(australianWeather$Date)


corMat=cor(australianWeather[,nums], method = c("pearson"),use = "complete.obs")


melted_corMat <- melt(corMat)

ggplot(data = melted_corMat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

gg_miss_var(australianWeather,show_pct = TRUE) + labs(y = "Percentage of Missing values")




```
We perform a special method of data imputation, following the timeseries plot of the WinDir9am, WindDir3pm and WindGustDir, we can see that if the last day was a certain category, it will probably be that same category. So we choose this as our method of imputation for categorical NAs.

```{r}
library(zoo)

australianWeatherTimeseriesPlot = australianWeather

australianWeatherTimeseriesPlot=filter(australianWeather, Location %in% "Albury")

data_cols = c("Date","WindDir9am","WindDir3pm","WindGustDir")

start_time = as.POSIXct("2010-01-01")
end_time = as.POSIXct("2011-01-01")

plot_grob=plot_timeline(australianWeatherTimeseriesPlot,data_cols = data_cols,start_time=start_time,end_time=end_time)

australianWeather$WindDir9am=na.locf(australianWeather$WindDir9am, fromLast = TRUE,na.rm=FALSE)
australianWeather$WindDir3pm=na.locf(australianWeather$WindDir3pm, fromLast = TRUE,na.rm=FALSE)
australianWeather$WindGustDir=na.locf(australianWeather$WindGustDir, fromLast = TRUE,na.rm=FALSE)

```


We remove the columns with over 30% NAs, as imputation might be too imprecise when over a third of data is missing, and dropping 30% of data might be too excesive. We also remove all NAs, which are 2% from RainToday and RainTomorrow, as RainTomorrow is the variable to predict, and any imputation will change the real space, and RainToday because it is highly rellated to RainTomorrow and might worsen our prediction.
To reduce the effect of the temporality of data we transform Date into the new variable Season, which is an approximation of the season to which the date belongs to.

```{r}
library(zoo)
library(caret)
library(tidyr)
gg_miss_var(australianWeather,show_pct = TRUE) + labs(y = "Percentage of Missing values")


australianWeather=australianWeather %>% drop_na(RainToday)
australianWeather=australianWeather %>% drop_na(RainTomorrow)


yq <- as.yearqtr(as.yearmon(australianWeather$Date, "%Y-%m-%d") + 1/12)


australianWeather$Season <- factor(format(yq, "%q"), levels = 1:4, 
                labels = c("winter", "spring", "summer", "fall"))

australianWeather <- subset (australianWeather, select = -Date)

summary(australianWeather)

australianWeather <- subset (australianWeather, select = -c(Evaporation,Sunshine,Cloud3pm,Cloud9am))

trainIndex <- createDataPartition(australianWeather$RainTomorrow, p = .8, 
                                  list = FALSE, 
                                  times = 1)
australianWeatherTrain <- australianWeather[ trainIndex,]
australianWeatherTest  <- australianWeather[-trainIndex,]
gg_miss_var(australianWeather,show_pct = TRUE) + labs(y = "Percentage of Missing values")


```
We perform the imputation of the missing continous data, however, to avoid data leakage from train into test, we separate the data into train and test, and build the imputation MICE predictive mean model on the train data, and apply it to both train and test.

```{r echo = T, results = 'hide'}

library(mice)
library(tidyr)

completeVector=c(1:nrow(australianWeather))

completeVector[trainIndex]=TRUE
completeVector[-trainIndex]=FALSE

cVec=!(!completeVector)

imputed <- mice(australianWeather, m=5,ignore = cVec, maxit = 5, method = 'pmm', seed = 500)

australianWeatherNoNA=complete(imputed,1)

gg_miss_var(australianWeatherNoNA,show_pct = TRUE) + labs(y = "Percentage of Missing values")

gg_miss_var(australianWeatherNoNA,show_pct = TRUE) + labs(y = "Percentage of Missing values")

```

We plot the density distributions of the data, we can observe a gaussian distribution in MinTemp, MaxTemp, Humidity3pm, Temp9am and Temp3pm. A mixture of gaussians can be observed in Humidity9am, and, if we consider each peak in the WindSpeed9am and WindSpeed3pm a gaussian, a extreme version of a mixture of gaussians is present in these variables. 
All the categorical variables, with the exception of RainTomorrow and RainToday have mostly equal distributions, the only major imbalance being in these two variables.

Rainfall does not conform to a Gaussian distribution, and a transformation must be applied specifically for it.

```{r}

library(gridExtra)
summary(australianWeatherNoNA)

g1 <- ggplot(australianWeatherNoNA, aes(x = Season)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")

g2 <- ggplot(australianWeatherNoNA, aes(x = WindGustDir)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g3 <- ggplot(australianWeatherNoNA, aes(x = WindDir9am)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g4 <- ggplot(australianWeatherNoNA, aes(x = WindDir3pm)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g5 <- ggplot(australianWeatherNoNA, aes(x = RainToday)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g6 <- ggplot(australianWeatherNoNA, aes(x = RainTomorrow)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")


g7 <- ggplot(australianWeatherNoNA, aes(x = MinTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g8 <- ggplot(australianWeatherNoNA, aes(x = MaxTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g9 <- ggplot(australianWeatherNoNA, aes(x = Rainfall)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g10 <- ggplot(australianWeatherNoNA, aes(x = WindSpeed9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g11 <- ggplot(australianWeatherNoNA, aes(x = WindSpeed3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g12 <- ggplot(australianWeatherNoNA, aes(x = Humidity9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g13 <- ggplot(australianWeatherNoNA, aes(x = Humidity3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g14 <- ggplot(australianWeatherNoNA, aes(x = Temp9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g15 <- ggplot(australianWeatherNoNA, aes(x = Temp3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )


grid.arrange(arrangeGrob(g1 + theme(legend.position="none"), g2 + theme(legend.position="none"),
g3 + theme(legend.position="none"),g4 + theme(legend.position="none"), g5 + theme(legend.position="none"), g6 + theme(legend.position="none"), g7 + theme(legend.position="none"), g8 + theme(legend.position="none"), g9 + theme(legend.position="none"), g10 + theme(legend.position="none"), g11 + theme(legend.position="none"), g12 + theme(legend.position="none"), g13 + theme(legend.position="none"), g14 + theme(legend.position="none"), g15 + theme(legend.position="none"),nrow=4),heights=c(10, 1))

```

A logarithmic transformation is applied to the rainfall variable, adding a constant value of 1 to deal with zeroes, this is to get Rainfall to a shape closer to a Gaussian, being the variable most far from a Gaussian distribution.

We scale the data to a mean of 0 and variance of 1, so as to be compatible with methods sensible to distance metrics.

```{r}

australianWeatherNoNA$Rainfall=log(australianWeatherNoNA$Rainfall+1)


scaled <- lapply(australianWeatherNoNA, function(x) if(is.numeric(x)){
                     scale(x, center=TRUE, scale=TRUE)
                      } else x)
scaled=as.data.frame(scaled)
```

Our new data retains its original shape with the exception of Rainfall, which, even when transformed, is still far away from a Gaussian distribution, but it is however, closer to it.

```{r}

library(gridExtra)
summary(scaled)

g1 <- ggplot(scaled, aes(x = Season)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")

g2 <- ggplot(scaled, aes(x = WindGustDir)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g3 <- ggplot(scaled, aes(x = WindDir9am)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g4 <- ggplot(scaled, aes(x = WindDir3pm)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g5 <- ggplot(scaled, aes(x = RainToday)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g6 <- ggplot(scaled, aes(x = RainTomorrow)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")


g7 <- ggplot(scaled, aes(x = MinTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g8 <- ggplot(scaled, aes(x = MaxTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g9 <- ggplot(scaled, aes(x = Rainfall)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g10 <- ggplot(scaled, aes(x = WindSpeed9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g11 <- ggplot(scaled, aes(x = WindSpeed3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g12 <- ggplot(scaled, aes(x = Humidity9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g13 <- ggplot(scaled, aes(x = Humidity3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g14 <- ggplot(scaled, aes(x = Temp9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g15 <- ggplot(scaled, aes(x = Temp3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )


grid.arrange(arrangeGrob(g1 + theme(legend.position="none"), g2 + theme(legend.position="none"),
g3 + theme(legend.position="none"),g4 + theme(legend.position="none"), g5 + theme(legend.position="none"), g6 + theme(legend.position="none"), g7 + theme(legend.position="none"), g8 + theme(legend.position="none"), g9 + theme(legend.position="none"), g10 + theme(legend.position="none"), g11 + theme(legend.position="none"), g12 + theme(legend.position="none"), g13 + theme(legend.position="none"), g14 + theme(legend.position="none"), g15 + theme(legend.position="none"),nrow=4),heights=c(10, 1))

```

While there appear to be some outliers, all the outliers in the boxplot almost in its entirety are extremely close together, suggesting highly skewed distributions, not outliers.

```{r}

nums <- unlist(lapply(scaled, is.numeric))  
ggplot(stack(scaled[,nums]), aes(x = ind, y = values)) +
  geom_boxplot()

```

Train and test sets are separated for further use in the classification section. The validation method to follow is as follows:

The dataset is divided into train and test, with the train and test data being imputed with the imputation model built from train data. The train data is to be divided into 10 folds for cross-validation, using cross-validation results to do model selection, at which point model validation over the model selected is performed with the test set.

```{r}
preprocessedTrain=scaled[trainIndex,]
preprocessedTest=scaled[-trainIndex,]

```

### Deal with computational complexity

Then, as a checkpoint for the pre-processing phase, the datasets have been saved on memory in order to avoid executing all the tmie this costly phase.
```{r}
write.csv(scaled,"scaled.csv", row.names = TRUE)
write.csv(preprocessedTrain,"scaledTrain.csv", row.names = TRUE)
write.csv(preprocessedTest,"scaledTest.csv", row.names = TRUE)
```

To make feasible in our computers the analysis below the dataset will be sampled all the time keeping `sampling_size` number of records (after reading it). To guarantee reproducibility of the sampling we used a seed `set.seed(1)`. The function `load_df` will be help in this operation (in particular in the **clustering** section).
```{r}
load_df <- function(sampling_size) {
  scaled <- read.csv('scaled.csv')
  
  # To have reproducibility of the sampling
  set.seed(1)
  
  # Random sampling
  scaled <- scaled[sample(nrow(scaled),sampling_size), ]
}
```

```{r}
library(FactoMineR)
load_df(1000)
```

# Visulization and Interpretation of the latent concepts
In this section, we apply LDA, PCA and MCA to visualize and interpret the latent concepts.

## Linear Discriminant Analysis
As we can observe the numerical variables (except location, wind direction, season) are nearly normally distributed, we use the training data to apply the Linear Discriminant Analysis and the response class is RainTomorrow.

```{r, message=FALSE}
library(MASS)
(model.lda <- lda(RainTomorrow~., data = preprocessedTrain[,-c(1,5,7,8,17,19)]))
```

Prior probabilities of groups defines the prior probability of the response classes for an observation. This shows 77.84 % of rain tomorrow and 22.16 % of not rain tomorrow.

Group Means defines the mean value for response classes. This indicates means values of different features when they fall to a particular response class. 

To be more specific, from the below diagram we see a clear difference between all the variables: all of them have opposite mean values for response class RainTomorrow. Especially for Humidity3pm, Humidity9am, Rainfall, Pressure9am, WindGustSpeed, their absolute values vary greatly. The more the difference between mean, the easier it will be to classify observation. 

```{r}
par(mar=c(7, 4.1, 4.1, 2.1))
barplot(model.lda$means, beside=TRUE, legend=TRUE, las=2, col=c("#FC4E07","#00AFBB"),  main = 'Group Means of LDA')
```

We can assume humidity, rainfall, pressure have more impact on the probabilities of rain on the next day; while temperature on 9am and minimum temperature have less impact. The more the humidity on 3pm and 9am, rainfall, speed of strongest wind, and less pressure on 3pm and 9am, the more likely it is to rain the next day.

### predictions
Next step, we find the model accuracy of 0.8423 for the training data, which is excellent.

```{r}
predmodel.train.lda = predict(model.lda, data=preprocessedTrain)
mean(predmodel.train.lda$class==preprocessedTrain$RainTomorrow)
table(Predicted=predmodel.train.lda$class, RainTomorrow=preprocessedTrain$RainTomorrow)
```

We check the posterior probabilities of a piece of data, we can find that the classifier basically meets our expectations.

```{r}
posteriors <- predmodel.train.lda$posterior 
head(posteriors) %>% 
  kbl(caption = "Posterior probabilities. RainAustralia data set") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

The below stacked histogram shows how the response class has been classified by the LDA classifier. The X-axis shows the value of line defined by the co-efficient of linear discriminant for LDA model. The two Yes/No groups are the groups for response class RainTomorrow.

```{r, message=FALSE}
ldahist(predmodel.train.lda$x[,1], g= predmodel.train.lda$class)
```

We check the model accuracy of 0.8389 for the test data, which is also excellent. 
```{r}
predmodel.test.lda = predict(model.lda, newdata=preprocessedTest)
mean(predmodel.test.lda$class==preprocessedTest$RainTomorrow)
table(Predicted=predmodel.test.lda$class, RainTomorrow=preprocessedTest$RainTomorrow)
```

The below figure shows how the data has been classified. The Predicted Group-No and Group-Yes has been colored with actual classification with red and blue color. The mix of color in the Group shows the incorrect classification prediction.

```{r}
par(mfrow=c(1,1))
plot(predmodel.test.lda$x[,1], predmodel.test.lda$class, col=ifelse(preprocessedTest$RainTomorrow=="No","#FC4E07","#00AFBB"))
```


## Principal Components Analysis
We apply PCA only on the numerical variables (already standardized in preprocessing phase).
``` {r, include=TRUE, results='hide'}
library(factoextra)
res.pca <- prcomp(scaled[,-c(1,5,7,8,17,18,19)], scale = TRUE)
```

```{r}
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#FC4E07", # Variables color
                col.ind = "yellow",  # Individuals color
                title="biplot - PCA",
                label="var",
                alpha.ind = 0.5)
```
As we can see, the first two dimensions explain more than 60% variance. The variables are approximately divided into 4 groups: temperature, pressure, wind speed and humidity (where rainfall is included in it). Each of these four groups of variables occupies a quadrant.

In the first dimension, temperature and wind speed have negative projection while pressure and humidity have positive projection. This means that there is a negative correlation between temperature, wind speed and pressure, humidity. The first principal component tells us about whether this observed day is with high pressure, wet, low temperature, low wind weather, or a low pressure, low humid and high temperature, windy day.

In the second dimension, MinTemp and Temp9am have little projection onto it. However, we can still observe that temperature, pressure (positive projection) are negatively correlated with wind speed and humidity (negative projection). Thus, this axis separates wet, windy day from high pressure, high temperature day.


## Multiple Correspondence Analysis

After analyzing the numerical variables, we use the categorical variables to apply MCA, using RainToday and RainTomorrow as supplementary variables.
```{r}
res.mca <- MCA(scaled[,c(1,5,7,8,17,18,19)],quali.sup=5:6, graph = FALSE)
```
```{r}
fviz_mca_var(res.mca, choice = "mca.cor", 
            repel = TRUE, # Avoid text overlapping (slow)
            ggtheme = theme_minimal())
```

We have very low explanation of variance (only 2.5% and 2.2% of the first and second dimension respectively). The reason may be the values of the categorical variables vary enormously. And we observe that supplementary variable RainTomorrow has slightly correlation with the first dimension, and RainToday has correlation with the second dimension. 


# Clustering

In the following chunk of code a tiny data pre processing will be applied to the dataset in order to prepare it to execute few clustering algorithms on top of it. To apply the clustering algorithms below the input dataset must be composed by **numeric variables**, therefore not numeric data will be discarded. The analysis will be performed considering just climatic descriptors.

```{r include=TRUE}
# Removing the first column describing the number of the row
loaded <- load_df(10000)
df <- loaded[2:ncol(loaded)]

# Keeping just numeric values
df <- df %>% dplyr::select(where(is.numeric))
```

## Partitioning method

The first approach with clustering method have been with the traditional partition methodology applying K-Means algorithm, since is the computationally less expensive technique studied. The algorithm have been executed, looking for 2, 3, 4 and 5 clusters (`centers = x`) in order to look for some likely shapes of the clusters.
It is plain that datas have the hape of a cloud, therefore it is not going to be possible distinguish clean clusters.

```{r}
library(mclust)
library(cluster)
```

```{r include=TRUE}
k2 <- kmeans(df, centers = 2, nstart = 25)
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

To determine the optimal number of clusters we adopted the **silhouette** method, with the respective code `method = "silhouette"`. The output suggest an optimal number of clusters equal to two.

```{r include=TRUE}
fviz_nbclust(df, kmeans, method = "silhouette")
```

The object of our analysis then will be based on this plot.

```{r include=TRUE}
fviz_cluster(k2, geom = "point", data = df)
```

As the silhouette method suggested will be studied the clustering with k equals to 2. For the interpretation of the obtained results, showing the centers `k2$centers` will help to associate each cluster to particular feature.
It is clear that the first cluster (1) is more representative for the high **temperature** sampling while the second cluster (2) is more representative for the low temperatures. High temperature cluster and low temperature cluster differ also in term of **humidity** and **pressure**, presenting lower values.
```{r include=TRUE}
k2$centers
```

Another trial to identify other kind of clusters shapes have been done applying a mixed approach, using a hierarchical clustering to determine the shape of clusters. The number of clusters will be specified by the parameter `k=4`. This time we will observe the characteristic of four different clusters.

```{r include=TRUE}
res.hk <- hkmeans(df, k=4)
fviz_cluster(res.hk, palette = "jco", repel = TRUE, ggtheme = theme_classic())
```

Adopting a higher number of cluster is easier to notice a higher variation in term of clusters specialization.
The most important cluster in this analysis is clearly the number (2) since it is represented by a high value of the `Rainfall` attribute and therefore it is representing the rainy days, that are very important for our analysis, since the goal of the following prediction phase will be focused on classify correctly the variable `Raintomorrow`. According with this cluster, rainy days are characterized by high wind values, low pressure and temperatures and high humidity.

```{r include=TRUE}
res.hk$centers
```

## Model Based Clustering

Since the biggest part of the dataset shows a gaussian distribution, a Gaussian finite mixture model fitted by EM algorithm should achieve good results in terms of clustering.

```{r include=TRUE}
mc<- Mclust(df)
fviz_mclust(mc, "uncertainty", palette = "jco")
```

Gaussian mixture produced as output 9 clusters of shape **VEV**. A such high number of cluster (9) suggest that, as we hypotized before, the shape of the data is a cloud point and that's why MBC is actually failing in findig clusters.
```{r include=TRUE}
mc$G
mc$modelName
```

## Hierarchical Clustering

Since the dataset doesn't shows explicit cluster so far, we decided to exploit the cloud shape of the dataset applying the hierarchical clustering. The metric chosen to compute distances is the `euclidean`. As before the `silhouette` method helped us to cut the tree to have the optimal number of clusters.

```{r include=TRUE}
# Removing the first column describing the number of the row
loaded <- load_df(1000)
df <- loaded[2:ncol(loaded)]

# Keeping just numeric values
df <- df %>% dplyr::select(where(is.numeric))
```

```{r include=TRUE}
d <- dist(df, method = "euclidean")
fviz_nbclust(df, FUN = hcut, method = "silhouette")
```

Then the hierarchical clustering algorithm have been executed considering the number of clusters equal to two (`k = 2`) allying all the known metrics to link clusters.

```{r include=TRUE}
hc.single   <- hclust(d, method="single")
hc.complete <- hclust(d, method="complete")
hc.average  <- hclust(d, method="average")
hc.ward     <- hclust(d, method="ward.D")

clust1 <- cutree(hc.single, k = 2)
clust2 <- cutree(hc.complete, k = 2)
clust3 <- cutree(hc.average, k = 2)
clust4 <- cutree(hc.ward, k = 2)
```

The plotted graphs actually don't show kind of new information we didn't observed in the previous analysis: As for the single and average method we can observe a clear connection (clustering) between the points on the extreme left, while the ward and complete linking method suggest a more clear separation between the top and the bottom. The analysis by mean of the ward method actually reminds the group observed with kmeans algorithm.

```{r include=TRUE}
p1 <- fviz_cluster(list(data = d, cluster = clust1)) + ggtitle("single")
p2 <- fviz_cluster(list(data = d, cluster = clust2)) + ggtitle("complete")
p3 <- fviz_cluster(list(data = d, cluster = clust3)) + ggtitle("average")
p4 <- fviz_cluster(list(data = d, cluster = clust4)) + ggtitle("ward.D")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```

# Classification

We load the preprocessed data and then we get off the "Location" variable to predict "RainTomorrow" independently from the specific location. 

```{r}
# Read imputed data
training_set <- read.csv("scaledTest.csv")
test_set <-read.csv("scaledTrain.csv")
```

We create a function that helps us to arrange both the test and training set which needs the "plyr" library for the "revalue" funcion.

```{r}
library(plyr)

arrange_data <- function(data){
                data=as.data.frame(data)
                # Omit Location column
                data=subset(data, select = -c(Location,X))
                # Ensure quantitative columns are numerical
                data$WindGustDir=as.factor(data$WindGustDir)
                data$WindDir9am=as.factor(data$WindDir9am)
                data$WindDir3pm=as.factor(data$WindDir3pm)
                # Convert Categorigal columns to Numerical with an integer encoding.
                data$RainTomorrow <- revalue(data$RainTomorrow, c("Yes"=1))
                data$RainTomorrow <- revalue(data$RainTomorrow, c("No"=0))
                data$RainToday <- revalue(data$RainToday, c("Yes"=1))
                data$RainToday <- revalue(data$RainToday, c("No"=0))
                data$Season  <- revalue(data$Season, c("winter"=1))
                data$Season  <- revalue(data$Season, c("spring"=2))
                data$Season  <- revalue(data$Season, c("summer"=3))
                data$Season  <- revalue(data$Season, c("fall"=4))
                data$RainToday  = as.factor(data$RainToday)
                data$RainTomorrow  = as.factor(data$RainTomorrow)
        return(data)
}

# Apply arrange function to both training and test set
training_set <- arrange_data(training_set)
test_set <- arrange_data(test_set)
```

```{r}
# @TODO:Comment that we have also tried to run everything with a balanced set ? # Take a balanced set
# training_set <-downSample(subset(training_set, select = -c(RainTomorrow)),training_set$RainTomorrow,list=FALSE,yname="RainTomorrow")

```

Once we have the training_set and test_set ready we randomly split the training set into 10 different folds by using "caret" library. We carry out the cross validation with the averaged results for each classification model.
```{r}
library(caret)
set.seed(123)
folds <- createFolds(training_set$RainTomorrow, k=10)
```
We create a function to automatize the calculation of the derived confussion matrix values such as the accuracy, precision, recall, f_score and selectivity. 
```{r}
# Importing library "dplyr" to use bind_rows() function
library(dplyr)
get_parameters <- function(cm, end_time){
    tp <- cm[1,1]
    tn <- cm[2,2]
    fp <- cm[2,1]
    fn <- cm[1,2]
    accuracy <- (cm[1,1] + cm[2,2])/ (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
    precision <- cm[1,1] / (cm[1,1] +  cm[2,1])
    recall <- cm[1,1] / (cm[1,1] + cm[1,2])
    f_score = (2 * precision * recall) / (precision + recall)
    selectivity <- cm[2,2]/ (cm[2,2] + cm[2,1])
    balan_accuracy <- (recall + selectivity)/2
    results <- data.frame(tp=c(tp),tn=c(tn),fp=c(fp),fn=c(fn),accuracy=c(accuracy),precision=c(precision),recall=c(recall),selectivity=c(selectivity),balan_accuracy=c(balan_accuracy),f_score=c(f_score),time=c(end_time))
    return(results)
    }
```

Now, we apply several classification models to the dataset for each fold and then we compute the averages of the confussion matrix metrics.

```{r}
# Prepare logistic regression function to be run for each fold
cvLogisticRegression <- lapply(folds, function(x){
  training_fold <- training_set[-x, ]
  test_fold <- training_set[x, ]
  start_time <- Sys.time()
  classifier <- glm(RainTomorrow ~ ., family = binomial, data = training_fold)
  end_time <- as.numeric(Sys.time() - start_time)
  y_pred <- predict(classifier, test_fold)
  y_pred <- ifelse(y_pred > 0.5, 1, 0)
  cm <- table(test_fold$RainTomorrow, y_pred)
  return(get_parameters(cm, end_time))
})
# Run Logistic regression for each fold and get the mean confussion matrix metrics 
cvLogisticRegression.res <-cbind(
  data.frame(model=c('Logistic regression')),
  as.data.frame(t(colMeans(bind_rows(cvLogisticRegression)))))
cvLogisticRegression.res
```


```{r}
# Prepare Nayve Bayes function to be run for each fold
library(e1071)
cvNaiveBayes <- lapply(folds, function(x){
  training_fold <- training_set[-x, ]
  test_fold <- training_set[x, ]
  start_time <- Sys.time()
  classifier <- naiveBayes( 
                x = subset(training_fold, select = -c(RainTomorrow)),
                y = training_fold$RainTomorrow)
  end_time <- as.numeric(Sys.time() - start_time)
  y_pred <- predict(classifier, subset(test_fold, select = -c(RainTomorrow)))
  cm <- table(test_fold$RainTomorrow, y_pred)
  return(get_parameters(cm, end_time))
})
# Run Naive Bayes model for each fold and get the mean confussion matrix metrics 
cvNaiveBayes.res <-cbind(
  data.frame(model=c('NaiveBayes')),
  as.data.frame(t(colMeans(bind_rows(cvNaiveBayes)))))
cvNaiveBayes.res
```

```{r}
# Prepare Decision Tree function to be run for each fold
library(rpart)
cvDecisionTree <- lapply(folds, function(x){
  training_fold <- training_set[-x, ]
  test_fold <- training_set[x, ]
  start_time <- Sys.time()
  classifier <- rpart(RainTomorrow ~ ., data = training_fold)
  end_time <- as.numeric(Sys.time() - start_time)
  y_pred <- predict(classifier, newdata = test_fold, type = 'class')
  cm <- table(test_fold$RainTomorrow, y_pred)
  return(get_parameters(cm, end_time))
})
# Run Decision Tree model for each fold and get the mean confussion matrix metrics 
cvDecisionTree.res <-cbind(
  data.frame(model=c('DecisionTree')),
  as.data.frame(t(colMeans(bind_rows(cvDecisionTree)))))
cvDecisionTree.res
```

```{r}
# Prepare Random Forest function to be run for each fold
library(randomForest)
cvRandomForest <- lapply(folds, function(x){
  training_fold <- training_set[-x, ]
  test_fold <- training_set[x, ]
  start_time <- Sys.time()
  classifier <- randomForest( 
                    x = subset(training_fold, select = -c(RainTomorrow)),
                    y = training_fold$RainTomorrow,
                    ntree = 300)
  end_time <- as.numeric(Sys.time() - start_time)
  y_pred <- predict(classifier, subset(test_fold, select = -c(RainTomorrow)))
  cm <- table(test_fold$RainTomorrow, y_pred)
  return(get_parameters(cm, end_time))
})

# Run RandomForest model for each fold and get the mean confussion matrix metrics 
cvRandomForest.res <-cbind(
  data.frame(model=c('RandomForest')),
  as.data.frame(t(colMeans(bind_rows(cvRandomForest)))))
cvRandomForest.res
```

Finally, we can compare the results of the classification models.
```{r}
# Row concatenation of results per each model
rbind(cvLogisticRegression.res,cvNaiveBayes.res,cvDecisionTree.res,cvRandomForest.res)
```
Looking at the table we can see that results are quite good in general. However, we have to take into account that our data was so unbalanced with about 80% of days with no rain. It means that we have to revise the different confussion matrix values to interpret the performance of the classification.

A typicall measure for model comparison is the accuracy, however in the case of imbalanced data it can be misleading. For this reason, we can use the balanced accuracy which somewhat normalizes true positive and true negative predictions by the total number of positive and negative divided by two. Therefore, accounting this metric we can conclude that the best model is Random Forest due to the fact that it has a higher value.

Note that we have included the total computation time to this table. We can see that despite being the best model, Random Forest is the slowest model, being Naibe Bayes the fastest one. Thus, in case we would need a fast model to train we could have chosen the second best model which is actually Naibe Bayes. 

In order to have the optimal classifier we need to perform a parameter tunning over Random Forest to optimize the "ntree" parameter which is the number of trees to grow during the algorithm. Hence, we are going to use again cross validation with the same 10 folds but now we are going to compare the performance of Random forests with the "ntree" values.

```{r}
# Function that runs Random forest for each fold with the input "ntree" value
parameter_tunning_RF <- function(ntree){
  mapply(function(x, ntree){
  training_fold <- training_set[-x,]
  test_fold <- training_set[x,]
  start_time <- Sys.time()
  classifier <- randomForest( 
                    x = subset(training_fold, select = -c(RainTomorrow)),
                    y = training_fold$RainTomorrow,
                    ntree = ntree)
  end_time <- as.numeric(Sys.time() - start_time)
  y_pred <- predict(classifier, subset(test_fold, select = -c(RainTomorrow)))
  cm <- table(test_fold$RainTomorrow, y_pred)
  return(get_parameters(cm, end_time))
  },ntree=ntree,x=folds)
}

# Function that executes the previous function for several "ntree" values
cvRandomForest_parameterTunning <- lapply(c(10,50,100,200,300,400,500), function(x){
  mid_result <- parameter_tunning_RF(x)
  means <- colMeans(as.data.frame(lapply(as.data.frame(t(mid_result)),as.numeric)))
  res <- cbind(data.frame(ntree=c(x)),t(means))
  return(res)
  })
# Concatenating rows per "ntree" value confussion matrix metrics
result <- bind_rows(cvRandomForest_parameterTunning)
```

Looking at the results of the cross validation we can consider that the value for the balanced accuracy is arround to 0.72 with the highest one on ntree=300. 

```{r}
result
```

Finally, we are going to train the final model, a Random Forest with 300 trees with the complete training set and we are going to test it with the complete test set.
# Final calculation with ntree = 300
```{r}
# Random Forest
start_time <- Sys.time()
classifier <- randomForest( 
                  x = subset(training_set, select = -c(RainTomorrow)),
                  y = training_set$RainTomorrow,
                  ntree = 300)
end_time <- as.numeric(Sys.time() - start_time)
y_pred <- predict(classifier, subset(test_set, select = -c(RainTomorrow)))
cm <- table(test_set$RainTomorrow, y_pred)
result_final <-get_parameters(cm,end_time)
result_final
```

The results shows that the model works pretty good having smothly a lower balanced accuracy than with the training set as expected. 
