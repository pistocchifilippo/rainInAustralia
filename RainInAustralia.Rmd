---
title: "RainInAustralia"
author: "Filippo, Antoni, Cristina, Mengxue"
date: "6/11/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.path='Figs/', echo=FALSE, warning=FALSE, message=FALSE)
```


```{r}
library(kableExtra)
library(stats)
```

Problem description:

The problem is to predict whether it will rain the next day or not based on measurements taken by the australian's government meteorological department. The measurements have been taken daily in various regions of Australia, with over 10 years of measurements. Due to the nature of the instruments used, some measurements are missing heavily in some areas, or are mostly incomplete. 

The purpose of our work is to find insights into the weather data, to see if we can ascend some patterns that might help better understand rain prediction.

Dataset description:

The dataset is compromised of 23 variables, and is a timeseries of australian weather, which the purpose of predicting whether it would rain tomorrow.

Date - Categorical variable, when the measurements were taken
Location - Categorical variable, where the measurements were taken
MinTemp - Numerical variable, minimal temperature observed that day
MaxTemp - Numerical variable, maximal temperature observed that day
Rainfall - Numerical variable, precipitation in the 24hours to 9am
Evaporation - Numerical variable, "Class A" pan evaporation in the 24 hours to 9am
Sunshine - Numerical variablem, bright sunshine hours in the 24 hours to midnight
WindGustDir - Categorical variable, direction of strongest gust in the 24 hours to midnight, 16 compass points
WindGustSpeed - Numerical variable, speed of strongest wind gust in the 24 hours to midnight
WindDir9am - Categorical variable, wind direction averaged over 10 minutes prior to 9 am
WindDir3pm - Categorical variable, wind direction averaged over 10 minutes prior to 3 pm
WindSpeed9am - Numerical variable, wind speed averaged over 10 minutes prior to 9 am
Windspeed3pm - Numerical variable, wind speed averaged over 10 minutes prior to 9 am
Humidity9am - Numerical variable, relative humidity at 9 am
Humidity3pm - Numerical variable, relative humidity at 3 pm
Pressure9am - Numerical variable, atmospheric pressure reduced to mean sea level at 9 am
Pressure3pm - Numerical variable, atmospheric pressure reduced to mean sea level at 3 pm
Cloud9am - Numerical variable, fraction of sky obscured by cloud at 9 am
Cloud3pm - Numerical variable, fraction of sky obscured by cloud at 3 pm
Temp9am - Numerical variable, temperature in Celsius at 9am
Temp3pm - Numerical variable, temperature in Celsius at 3pm
RainToday - Categorical variable, whether it rained today or not
RainTomorrow - Categorical variable, whether tomorrow will rain or not

```{r}

set.seed(1)

australianWeather <- read.csv(file = 'weatherAUS.csv')
```


We perform a basic visualization, first of the correlation between variables, which isn't significant with the exception of variables recorded in the same day, that is, those measurements taken at 9am and 3pm, this helps us see that there's an important temporal component in the same day.
```{r}
library(naniar)
library(ggplot2)
library(reshape2)
library(dplyr)
library(timelineR)
library(tseries)

nums <- unlist(lapply(australianWeather, is.numeric))  

australianWeather$Date=as.POSIXct(australianWeather$Date)


corMat=cor(australianWeather[,nums], method = c("pearson"),use = "complete.obs")


melted_corMat <- melt(corMat)

ggplot(data = melted_corMat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

gg_miss_var(australianWeather,show_pct = TRUE) + labs(y = "Percentage of Missing values")




```
We perform a special method of data imputation, following the timeseries plot of the WinDir9am, WindDir3pm and WindGustDir, we can see that if the last day was a certain category, it will probably be that same category. So we choose this as our method of imputation for categorical NAs.

```{r}
library(zoo)

australianWeatherTimeseriesPlot = australianWeather

australianWeatherTimeseriesPlot=filter(australianWeather, Location %in% "Albury")

data_cols = c("Date","WindDir9am","WindDir3pm","WindGustDir")

start_time = as.POSIXct("2010-01-01")
end_time = as.POSIXct("2011-01-01")

plot_grob=plot_timeline(australianWeatherTimeseriesPlot,data_cols = data_cols,start_time=start_time,end_time=end_time)

australianWeather$WindDir9am=na.locf(australianWeather$WindDir9am, fromLast = TRUE,na.rm=FALSE)
australianWeather$WindDir3pm=na.locf(australianWeather$WindDir3pm, fromLast = TRUE,na.rm=FALSE)
australianWeather$WindGustDir=na.locf(australianWeather$WindGustDir, fromLast = TRUE,na.rm=FALSE)

```


We remove the columns with over 30% NAs, as imputation might be too imprecise when over a third of data is missing, and dropping 30% of data might be too excesive. We also remove all NAs, which are 2% from RainToday and RainTomorrow, as RainTomorrow is the variable to predict, and any imputation will change the real space, and RainToday because it is highly rellated to RainTomorrow and might worsen our prediction.
To reduce the effect of the temporality of data we transform Date into the new variable Season, which is an approximation of the season to which the date belongs to.

```{r}
library(zoo)
library(caret)
library(tidyr)
gg_miss_var(australianWeather,show_pct = TRUE) + labs(y = "Percentage of Missing values")


australianWeather=australianWeather %>% drop_na(RainToday)
australianWeather=australianWeather %>% drop_na(RainTomorrow)


yq <- as.yearqtr(as.yearmon(australianWeather$Date, "%Y-%m-%d") + 1/12)


australianWeather$Season <- factor(format(yq, "%q"), levels = 1:4, 
                labels = c("winter", "spring", "summer", "fall"))

australianWeather <- subset (australianWeather, select = -Date)

summary(australianWeather)

australianWeather <- subset (australianWeather, select = -c(Evaporation,Sunshine,Cloud3pm,Cloud9am))

trainIndex <- createDataPartition(australianWeather$RainTomorrow, p = .8, 
                                  list = FALSE, 
                                  times = 1)
australianWeatherTrain <- australianWeather[ trainIndex,]
australianWeatherTest  <- australianWeather[-trainIndex,]
gg_miss_var(australianWeather,show_pct = TRUE) + labs(y = "Percentage of Missing values")


```
We perform the imputation of the missing continous data, however, to avoid data leakage from train into test, we separate the data into train and test, and build the imputation MICE predictive mean model on the train data, and apply it to both train and test.

```{r}

library(mice)
library(tidyr)




completeVector=c(1:nrow(australianWeather))

completeVector[trainIndex]=TRUE
completeVector[-trainIndex]=FALSE

cVec=!(!completeVector)

imputed <- mice(australianWeather, m=5,ignore = cVec, maxit = 5, method = 'pmm', seed = 500)

australianWeatherNoNA=complete(imputed,1)


gg_miss_var(australianWeatherNoNA,show_pct = TRUE) + labs(y = "Percentage of Missing values")



gg_miss_var(australianWeatherNoNA,show_pct = TRUE) + labs(y = "Percentage of Missing values")


```

We plot the density distributions of the data, we can observe a gaussian distribution in MinTemp, MaxTemp, Humidity3pm, Temp9am and Temp3pm. A mixture of gaussians can be observed in Humidity9am, and, if we consider each peak in the WindSpeed9am and WindSpeed3pm a gaussian, a extreme version of a mixture of gaussians is present in these variables. 
All the categorical variables, with the exception of RainTomorrow and RainToday have mostly equal distributions, the only major imbalance being in these two variables.

Rainfall does not conform to a Gaussian distribution, and a transformation must be applied specifically for it.
```{r}

library(gridExtra)
summary(australianWeatherNoNA)

g1 <- ggplot(australianWeatherNoNA, aes(x = Season)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")

g2 <- ggplot(australianWeatherNoNA, aes(x = WindGustDir)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g3 <- ggplot(australianWeatherNoNA, aes(x = WindDir9am)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g4 <- ggplot(australianWeatherNoNA, aes(x = WindDir3pm)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g5 <- ggplot(australianWeatherNoNA, aes(x = RainToday)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g6 <- ggplot(australianWeatherNoNA, aes(x = RainTomorrow)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")


g7 <- ggplot(australianWeatherNoNA, aes(x = MinTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g8 <- ggplot(australianWeatherNoNA, aes(x = MaxTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g9 <- ggplot(australianWeatherNoNA, aes(x = Rainfall)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g10 <- ggplot(australianWeatherNoNA, aes(x = WindSpeed9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g11 <- ggplot(australianWeatherNoNA, aes(x = WindSpeed3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g12 <- ggplot(australianWeatherNoNA, aes(x = Humidity9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g13 <- ggplot(australianWeatherNoNA, aes(x = Humidity3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g14 <- ggplot(australianWeatherNoNA, aes(x = Temp9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g15 <- ggplot(australianWeatherNoNA, aes(x = Temp3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )


grid.arrange(arrangeGrob(g1 + theme(legend.position="none"), g2 + theme(legend.position="none"),
g3 + theme(legend.position="none"),g4 + theme(legend.position="none"), g5 + theme(legend.position="none"), g6 + theme(legend.position="none"), g7 + theme(legend.position="none"), g8 + theme(legend.position="none"), g9 + theme(legend.position="none"), g10 + theme(legend.position="none"), g11 + theme(legend.position="none"), g12 + theme(legend.position="none"), g13 + theme(legend.position="none"), g14 + theme(legend.position="none"), g15 + theme(legend.position="none"),nrow=4),heights=c(10, 1))

```

A logarithmic transformation is applied to the rainfall variable, adding a constant value of 1 to deal with zeroes, this is to get Rainfall to a shape closer to a Gaussian, being the variable most far from a Gaussian distribution.

We scale the data to a mean of 0 and variance of 1, so as to be compatible with methods sensible to distance metrics.
```{r}

australianWeatherNoNA$Rainfall=log(australianWeatherNoNA$Rainfall+1)


scaled <- lapply(australianWeatherNoNA, function(x) if(is.numeric(x)){
                     scale(x, center=TRUE, scale=TRUE)
                      } else x)
scaled=as.data.frame(scaled)
```

Our new data retains its original shape with the exception of Rainfall, which, even when transformed, is still far away from a Gaussian distribution, but it is however, closer to it.
```{r}

library(gridExtra)
summary(scaled)

g1 <- ggplot(scaled, aes(x = Season)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")

g2 <- ggplot(scaled, aes(x = WindGustDir)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g3 <- ggplot(scaled, aes(x = WindDir9am)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g4 <- ggplot(scaled, aes(x = WindDir3pm)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g5 <- ggplot(scaled, aes(x = RainToday)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")
g6 <- ggplot(scaled, aes(x = RainTomorrow)) +
  geom_bar(alpha = 0.7) + theme_bw() +
  theme(legend.position="bottom")


g7 <- ggplot(scaled, aes(x = MinTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g8 <- ggplot(scaled, aes(x = MaxTemp)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position="none")
g9 <- ggplot(scaled, aes(x = Rainfall)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g10 <- ggplot(scaled, aes(x = WindSpeed9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g11 <- ggplot(scaled, aes(x = WindSpeed3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g12 <- ggplot(scaled, aes(x = Humidity9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g13 <- ggplot(scaled, aes(x = Humidity3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g14 <- ggplot(scaled, aes(x = Temp9am)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )
g15 <- ggplot(scaled, aes(x = Temp3pm)) +
  geom_density(alpha = 0.7) + theme_bw() +
  theme(legend.position ="none" )


grid.arrange(arrangeGrob(g1 + theme(legend.position="none"), g2 + theme(legend.position="none"),
g3 + theme(legend.position="none"),g4 + theme(legend.position="none"), g5 + theme(legend.position="none"), g6 + theme(legend.position="none"), g7 + theme(legend.position="none"), g8 + theme(legend.position="none"), g9 + theme(legend.position="none"), g10 + theme(legend.position="none"), g11 + theme(legend.position="none"), g12 + theme(legend.position="none"), g13 + theme(legend.position="none"), g14 + theme(legend.position="none"), g15 + theme(legend.position="none"),nrow=4),heights=c(10, 1))

```

While there appear to be some outliers, all the outliers in the boxplot almost in its entirety are extremely close together, suggesting highly skewed distributions, not outliers.
```{r}

nums <- unlist(lapply(scaled, is.numeric))  
ggplot(stack(scaled[,nums]), aes(x = ind, y = values)) +
  geom_boxplot()

```

Train and test sets are separated for further use in the classification section. The validation method to follow is as follows:

The dataset is divided into train and test, with the train and test data being imputed with the imputation model built from train data. The train data is to be divided into 10 folds for cross-validation, using cross-validation results to do model selection, at which point model validation over the model selected is performed with the test set.

```{r}
preprocessedTrain=scaled[trainIndex,]
preprocessedTest=scaled[-trainIndex,]

```

### Deal with computational complexity

Then, as a checkpoint for the pre-processing phase, the datasets have been saved on memory in order to avoid executing all the tmie this costly phase.
```{r}
write.csv(scaled,"scaled.csv", row.names = TRUE)
write.csv(preprocessedTrain,"scaledTrain.csv", row.names = TRUE)
write.csv(preprocessedTest,"scaledTest.csv", row.names = TRUE)
```

To make feasible in our computers the analysis below the dataset will be sampled all the time keeping 1000 records (after reading it). To guarantee reproducibility of the sampling we used a seed `set.seed(1)`. The function `load_df` will be help in this operation.
```{r}
load_df <- function(sampling_size) {
  scaled <- read.csv('scaled.csv')
  
  # To have reproducibility of the sampling
  set.seed(1)
  
  # Random sampling
  scaled <- scaled[sample(nrow(scaled),sampling_size), ]
}
```

# Visualization
```{r}
library(FactoMineR)
load_df(1000)
```

## LDA
Firstly we use numerical variables (except location, wind direction, season) to apply LDA.
```{r, message=FALSE}
library(MASS)
(model.lda <- lda(RainTomorrow~., data = scaled[,-c(1,5,7,8,17,19)]))
```

Prior probabilities of groups defines the prior probability of the response classes for an observation. This shows 77.84 % of rain tomorrow and 22.16 % of not rain tomorrow.

Group Means defines the mean value (Âµk) for response classes for a particular X=x. This indicates means values of different features when they fall to a particular response class. 

We see a clear difference between all the variables: they have opposite mean values for class RainTomorrow. Especially for Humidity3pm, Humidity9am, Rainfall,Pressure9am, their absolute values vary greatly. The more the difference between mean, the easier it will be to classify observation. We can assume humidity, rainfall, pressure have more impact on the probabilities of rain on the second day; while temperature on 9am and minimum temperature have less impact.
```{r}
par(mar=c(7, 4.1, 4.1, 2.1))
barplot(model.lda$means, beside=TRUE, legend=TRUE, las=2, col=c("#FC4E07","#00AFBB"))
```


### predictions

```{r}
##Predicting training results.
prediction = predict(model.lda, data=scaled)
mean(prediction$class==scaled$RainTomorrow)
table(Predicted=prediction$class, RainTomorrow=scaled$RainTomorrow)
```

The below plot shows how the response class has been classified by the LDA classifier. The X-axis shows the value of line defined by the co-efficient of linear discriminant for LDA model. The two groups are the groups for response classes.
```{r, message=FALSE}
ldahist(prediction$x[,1], g= prediction$class)
```

The below figure shows how the data has been classified. The Predicted Group-No and Group-Yes has been colored with actual classification with red and blue color. The mix of color in the Group shows the incorrect classification prediction.

```{r}
par(mfrow=c(1,1))
plot(prediction$x[,1], prediction$class, col=ifelse(scaled$RainTomorrow=="No","#FC4E07","#00AFBB"))
```


## PCA
Applying PCA only on the numerical variables (already standardized in preprocessing phase).

``` {r, include=TRUE, results='hide'}
library(factoextra)
res.pca <- prcomp(scaled[,-c(1,5,7,8,17,18,19)], scale = TRUE)
```

```{r}
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#FC4E07", # Variables color
                col.ind = "yellow",  # Individuals color
                title="biplot - PCA",
                label="var",
                alpha.ind = 0.5)
```

## MFA 
Dividing variables into 8 group.

```{r}
res.mfa <- MFA(scaled[,c(1,2,3,15,16,5,7,8,6,9,10,11,12,13,14,17,18,19)],group=c(1,4,3,3,2,2,2,1),type=c("n","s","n",rep("s",3),rep("n",2)),name.group=c("Location","Temperature","WinDir","WinSpeed","Humidity","Pressure", "RainToday/Tomorrow", "Season"))
```


## MCA

Only use categorical variables to apply mca, RainToday and RainTomorrow as supplementary variables.

```{r}
res.mca <- MCA(scaled[,c(1,5,7,8,17,18,19)],quali.sup=5:6)
```

#Clustering

In the following chunk of code a tiny data pre processing will be applied to the dataset in order to prepare it to execute few clustering algorithms on top of it. To apply the clustering algorithms below the input dataset must be composed by **numeric variables**, therefore not numeric data will be discarded. The analysis will be performed considering just climatic descriptors.

```{r}
# Removing the first column describing the number of the row
loaded <- load_df(10000)
df <- loaded[2:ncol(loaded)]

# Keeping just numeric values
df <- df %>% dplyr::select(where(is.numeric))
```

## Partitioning method

The first approach with clustering method have been with the traditional partition methodology applying K-Means algorithm, since is the computationally less expensive technique studied. The algorithm have been executed, looking for 2, 3, 4 and 5 clusters (`centers = x`) in order to look for some likely shapes of the clusters.
It is plain that datas have the hape of a cloud, therefore it is not going to be possible distinguish clean clusters.

```{r}
library(mclust)
library(cluster)

k2 <- kmeans(df, centers = 2, nstart = 25)
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)

p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

To determine the optimal number of clusters we adopted the **silhouette** method, with the respective code `method = "silhouette"`. The output suggest an optimal number of clusters equal to two.

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")
```

The object of our analysis then will be based on this plot.

```{r}
fviz_cluster(k2, geom = "point", data = df)
```

As the silhouette method suggested will be studied the clustering with k equals to 2. For the interpretation of the obtained results, showing the centers `k2$centers` will help to associate each cluster to particular feature.
It is clear that the first cluster (1) is more representative for the high **temperature** sampling while the second cluster (2) is more representative for the low temperatures. High temperature cluster and low temperature cluster differ also in term of **humidity** and **pressure**, presenting lower values.
```{r}
k2$centers
```

Another trial to identify other kind of clusters shapes have been done applying a mixed approach, using a hierarchical clustering to determine the shape of clusters. The number of clusters will be specified by the parameter `k=4`. This time we will observe the characteristic of four different clusters.

```{r}
res.hk <- hkmeans(df, k=4)
fviz_cluster(res.hk, palette = "jco", repel = TRUE, ggtheme = theme_classic())
```

Adopting a higher number of cluster is easier to notice a higher variation in term of clusters specialization.
The most important cluster in this analysis is clearly the number (2) since it is represented by a high value of the `Rainfall` attribute and therefore it is representing the rainy days, that are very important for our analysis, since the goal of the following prediction phase will be focused on classify correctly the variable `Raintomorrow`. According with this cluster, rainy days are characterized by high wind values, low pressure and temperatures and high humidity.
```{r}
res.hk$centers
```

## Model Based Clustering

Since the biggest part of the dataset shows a gaussian distribution, a Gaussian finite mixture model fitted by EM algorithm should achieve good results in terms of clustering.

```{r}
mc<- Mclust(df)
fviz_mclust(mc, "uncertainty", palette = "jco")
```

Gaussian mixture produced as output 9 clusters of shape **VEV**. A such high number of cluster (9) suggest that, as we hypotized before, the shape of the data is a cloud point and that's why MBC is actually failing in findig clusters.
```{r}
mc$G
mc$modelName
```

## Hierarchical Clustering

Since the dataset doesn't shows explicit cluster so far, we decided to exploit the cloud shape of the dataset applying the hierarchical clustering. The metric chosen to compute distances is the `euclidean`. As before the `silhouette` method helped us to cut the tree to have the optimal number of clusters.

```{r}
# Removing the first column describing the number of the row
loaded <- load_df(1000)
df <- loaded[2:ncol(loaded)]

# Keeping just numeric values
df <- df %>% dplyr::select(where(is.numeric))
```

```{r}
d <- dist(df, method = "euclidean")
fviz_nbclust(df, FUN = hcut, method = "silhouette")
```

Then the hierarchical clustering algorithm have been executed considering the number of clusters equal to two (`k = 2`) allying all the known metrics to link clusters.
```{r}
hc.single   <- hclust(d, method="single")
hc.complete <- hclust(d, method="complete")
hc.average  <- hclust(d, method="average")
hc.ward     <- hclust(d, method="ward.D")

clust1 <- cutree(hc.single, k = 2)
clust2 <- cutree(hc.complete, k = 2)
clust3 <- cutree(hc.average, k = 2)
clust4 <- cutree(hc.ward, k = 2)
```

The plotted graphs actually don't show kind of new information we didn't observed in the previous analysis: As for the single and average method we can observe a clear connection (clustering) between the points on the extreme left, while the ward and complete linking method suggest a more clear separation between the top and the bottom. The analysis by mean of the ward method actually reminds the group observed with kmeans algorithm.

```{r}
p1 <- fviz_cluster(list(data = d, cluster = clust1)) + ggtitle("single")
p2 <- fviz_cluster(list(data = d, cluster = clust2)) + ggtitle("complete")
p3 <- fviz_cluster(list(data = d, cluster = clust3)) + ggtitle("average")
p4 <- fviz_cluster(list(data = d, cluster = clust4)) + ggtitle("ward.D")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```